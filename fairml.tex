%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

%\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
\documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% Additional packages
\usepackage{epstopdf}
\usepackage{url}
\usepackage{breakurl}
\usepackage[breaklinks]{hyperref}
\usepackage{wrapfig}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Machine Learning}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Towards Automated Bias Detection and Mitigation in Machine Learning}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author{Alfa Yohannis, Dimitris Kolovos}

\affiliation{organization={Department of Computer Science, University of York},%Department and Organization
%            addressline={}, 
            city={York},
%            postcode={}, 
%            state={},
            country={United Kingdom}}

\begin{abstract}
Models produced by machine learning are not guaranteed free from bias, particularly when trained and tested with data produced in discriminatory environments. The bias can be unethically acceptable, especially when the data contains sensitive attributes, such as sex, race, age, etc. Some approaches have contributed to mitigating such bias by providing bias metrics and mitigation algorithms, and they are already supported by most fair machine learning toolkits. FairML brings a model-based approach to improve the fairness of machine learning by levelling up the abstraction and automatically generating the code and reports of bias measurement and mitigation.
\end{abstract}

%%Graphical abstract
\begin{graphicalabstract}
%\includegraphics{grabs}
\end{graphicalabstract}

%%Research highlights
\begin{highlights}
\item Research highlight 1
\item Research highlight 2
\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:introduction}
The use of machine learning is pervasive nowadays, from personal daily activities, such as face recognition for authentication and voice processing when talking to smart assistants (e.g., Alexa, Siri), to performing sensitive, ethical tasks, such as predicting criminals and classifying profiles for loans. While machine learning does bring efficiencies, models produced by machine learning are not guaranteed free from bias, particularly when trained and tested with data produced in discriminatory environments. The bias can be unethically acceptable, particularly when it touches sensitive attributes, such as sex, race, age, etc., and cause unfairness. 

In 2016, an algorithm for predicting recidivism was found produced high false negative rate for white people and high false positive rate for black people \cite{angwin2016machine}. Some commercial face recognition services were also found having significant lower accuracy on darker-skin females \cite{buolamwini2018gender}. Moreover, A job platform was found to rank qualified female candidates far lower than qualified male candidates even though they have similar properties \cite{lahoti2019ifair}. These are some of many other instances that show biases in machine learning can promote unfairness. 

Some approaches have contributed to the mitigation of such biases by providing metrics []  and algorithms [] to measure and mitigate biases. Some toolkits do exist to allow users to implement such metrics and algorithms, such as IBM AI Fairness 
360\footnote{\url{https://aif360.mybluemix.net/}}, Fairlearn\footnote{\url{https://fairlearn.org/}}, Google What-If\footnote{\url{https://pair-code.github.io/what-if-tool/}}, Aequitas\footnote{\url{http://aequitas.dssg.io/}}.
However, these toolkits come with different methodological approaches and capabilities which users need to understand so that they can decide which toolkits are best for certain scenarios~\cite{lee2021landscape}.  

Data scientists usually work using their intuitions to narrow down the number of combinations of algorithms, parameters, and other factors in order to find the best models for given goals, datasets, and domains \cite{muller2016introduction}. After that, with lots of experimentation and trial and error \cite{byrne2017development}, they have to go through all the narrowed combinations and test the produced models to identify which models are the best. Moreover, regardless of the availability of machine learning libraries, data scientists have to craft the search process from scratch in programming languages (e.g, Python, R).

In software engineering, Model-Driven Sofware Development (MDSE) is a development paradigm of software in which models are used as the primary artefacts that drive the process of software development. The models are used as bases/references to (semi)automatically generate the target software \cite{brambilla2017model}. MDSE aims for a productive environment by speeding up development speed through artefact generation and managing complexity by raising up abstraction level in the form of simpler modelling languages \cite{volter2013model}. The use of MDSE would benefit data scientists since it allows them to search for the best bias mitigation methods for given cases at a higher abstraction level. They also do not have to code the implementation of the search process in general/statistical programming languages since it can be automatically generated and then fine tuned later on. 

The contribution of this research is FairML, a tool that implements MDSE approach to model and automate measuring and mitigating biases in machine learning. 
\begin{itemize}
\item FairML raises up the abstraction of bias measurement and mitigation so that users can configure their bias mitigation model in YAML (YAML Ain’t Markup Language), a human-friendly declarative language \cite{evans2017yaml}, without having to code in general/statistical programming languages.
\item FairML supports certain degree of expressiveness which allows users to experiment with different kinds of bias metrics, bias mitigation algorithms, datasets, classifiers, and their parameters to find the best of their combinations that reduces biases but with acceptable accuracy.
\item It automatically generates Python and Jupyter Notenbook files which users can execute to run measure and mitigate biases on given datasets. All generated files are modifiable and extensible for fine tuning and further development.
\end{itemize}



\section{Bias in Machine Learning}
\label{sec:bias_in_machine_learning}

\subsection{Definitions and Examples}
\label{sec:definitions_and_examples}

Fairness in defined as ``the absence of any prejudice or favoritism toward an individual or
group based on their inherent or acquired characteristics'' \cite{mehrabi2021survey}.
The absence of fairness can be caused by bias which is a systematic error or distortion from the true state of affairs due to flaws in data collection and processing, study design, analysis, and interpretation \cite{oxford2022bias}. 
The unfairness happens when biases put privileged groups at the advantaged position against unprivileged groups \cite{bellamy2018ai}. 
Bias can also be caused by negative discrimination if the distinction made is due to intentional or unintentional stereotyping and prejudice based on sensitive attributes (race, age, sex, etc.) \cite{mehrabi2021survey,chen2019fairness}. 






A Survey on Bias and Fairness in Machine Learning

It has been found in 2016 that COMPAS, the algorithm used for recidivism prediction produces much higher false positive rate for black people than white people \cite{angwin2016machine}.

XING, a job platform similar to Linked-in, was found to rank less qualified male candidates higher than more qualified female candidates \cite{lahoti2019ifair}.

Publicly available commercial face recognition online services provided by Microsoft, Face++, and IBM respectively are found to suffer from achieving much lower accuracy on females with darker skin color \cite{buolamwini2018gender}.

\subsection{Bias Metrics}
\label{sec:bias_metrics}



Statistical parity Difference \cite{dwork2012fairness,bellamy2018ai} is a metric computed as the difference of the rate of favourable outcomes received by the unprivileged group to the privileged group. The ideal value of this metric is 0. Fairness for this metric is between -0.1 and 0.1.

Equal Opportunity Difference \cite{bellamy2018ai} is a metric computed as the difference of true positive rates between the unprivileged and the privileged groups. 
The true positive rate is the ratio of true positives to the total number of actual positives for a given group. The ideal value is 0. A value of $<$ 0 implies higher benefit for the privileged group and a value $>$ 0 implies higher benefit for the unprivileged group. Fairness for this metric is between -0.1 and 0.1.

Average Odds Difference \cite{bellamy2018ai} is a metric computed as average difference of false positive rate (false positives / negatives) and true positive rate (true positives / positives) between unprivileged and privileged groups.
The ideal value of this metric is 0. A value of $<$ 0 implies higher benefit for the privileged group and a value $>$ 0 implies higher benefit for the unprivileged group.
Fairness for this metric is between -0.1 and 0.1

Disparate Impact \cite{feldman2015disparate,bellamy2018ai} is a metric computed as the ratio of rate of favourable outcome for the unprivileged group to that of the privileged group.
The ideal value of this metric is 1.0 A value $<$ 1 implies higher benefit for the privileged group and a value $>$1 implies a higher benefit for the unprivileged group.
Fairness for this metric is between 0.8 and 1.25.

5. Theil Index \cite{conceicao2000theyoung,bellamy2018ai}
Computed as the generalized entropy of benefit for all individuals in the dataset, with alpha = 1. It measures the inequality in benefit allocation for individuals.
A value of 0 implies perfect fairness. Fairness is indicated by lower scores, higher scores are problematic.


 Fairness Measures (Zehlike et al., 2017), for example,
provides several fairness metrics, including difference of
means, disparate impact, and odds ratio. A set of datasets
is also provided, though some datasets are not in the pub-
lic domain and need explicit permission from the owners
to access/use the data. Similarly, FairML (Adebayo, 2016)
provides an auditing tool for predictive models by quantify-
ing the relative effects of various inputs on a models predic-
tions. This, in turn, can be used to assess the models fair-
ness. FairTest (Tram`

er et al., 2017), on the other hand, ap-
proaches the task of detecting biases in a dataset by check-
ing for associations between predicted labels and protected
attributes. The methodology also provides a way to iden-
tify regions of the input space where an algorithm might
incur unusually high errors. This toolkit also includes a
rich catalog of datasets. Aequitas (Stevens et al., 2018) is
another auditing toolkit for data scientists as well as pol-
icy makers; it has a Python library as well as an associated
web site where data can be uploaded for bias analysis. It
offers several fairness metrics, including demographic or
statistical parity and disparate impact, along with a ”fair-
ness tree” to help users identify the correct metric to use for
their particular situation. Aequitas’s license does not allow


AI Fairness 360

commercial use. Finally, Themis (Galhotra et al., 2017) is
an open source bias toolbox that automatically generates
test suites to measure discrimination in decisions made by
a predictive system.

\subsection{Bias Mitigation}
\label{sec:bias_mitigation}

A handful of toolkits address both bias detection as well
as bias mitigation. Themis-ML (Bantilan, 2018) is one
such repository that provides a few fairness metrics, such
as mean difference, as well as some bias mitigation algo-
rithms, such as relabeling (Kamiran \& Calders, 2012), ad-
ditive counterfactually fair estimator (Kusner et al., 2017),
and reject option classiﬁcation (Kamiran et al., 2012). The
repository contains a subset of the methods described in the
paper. Fairness Comparison (Friedler et al., 2018) is one of
the more extensive libraries. It includes several bias detec-
tion metrics as well as bias mitigation methods, including
disparate impact remover (Feldman et al., 2015), prejudice
remover (Kamishima et al., 2012), and two-Naive Bayes
(Calders \& Verwer, 2010). Written primarily as a test-bed
to allow different bias metrics and algorithms to be com-
pared in a consistent way, it also allows the addition of ad-
ditional algorithms and datasets.

\subsection{Toolkits}
\label{sec:toolkits}

Lee et al. \cite{lee2021landscape} studied the 

\section{Model-based Software Development}
\label{sec:model_based_software_development}
In software engineering, Model-Driven Sofware Development (MDSE) is a development paradigm of software in which models are used as the primary artefacts to drive the process of software development. The models are used as bases/references to (semi)automatically generate the target software \cite{brambilla2017model}. MDSE aims for a productive environment by speeding up development speed through artefact generation and managing complexity by raising up abstraction level in the form of simpler modelling languages \cite{volter2013model}.

\section{FairML}
\label{sec:fairml}

\subsection{Domain Analysis}
\label{sec:domain_analysis}
In order to model bias measurement and mitigation in machine learning, important constructs of the domain should be represented in the metamodel. Thus, we analysed how the toolkits in Section \ref{sec:toolkits} are implemented in their code repositories. The analysis covers identifying constructs and  their order in demos and examples and gathering all parameters required for each class and method.
We identify that, in general, all the toolkits perform similar activities and can be grouped into the phases (preprocessing, inprocessing, postprocessing) where they are applied in the machine learning pipeline. 

Every processing starts with a task setting up a dataset. In the task, users define the source of the dataset which is commonly a CSV file. Users also determine the target/predicted attribute, the favourable class in the predicted attribute, the sensitive attributes as well as the privileged and unprivileged classes in the ssensitive attributes. 

Bias mitigation in \textit{preprocessing} can be done without training any model and performing any prediction since the transformation to reduce biases is only applied to datasets. However, if users need to check the accuracy of the model trained from original and after-mitigation datasets then training and prediction have to be performed as well.

Bias mitigation in \textit{inprocessing} requires users to train the data and make prediction. 

\begin{itemize}
	\item \textbf{Preprocessing}. Mitigating bias in preprocessing starts with setting up the dataset. In the setup, users choose which 
	\item \textbf{Preprocessing}. Mitigating bias in preprocessing starts with settting up the dataset. 
	\item \textbf{Preprocessing}. Mitigating bias in preprocessing starts with setting up the dataset. 
\end{itemize}



\begin{enumerate}
\item Define Datasets. 
\item Measure Bias. 
\item Mitigate Bias.
\item Mitigate Bias .
\item Train Model.
\item 

\end{enumerate}

\subsection{Design}
\label{sec:design}

\subsection{Use Cases}
\label{sec:use_Cases}

\section{Evaluation}
\label{sec:evaluation}



Ratio number of lines written : generated

Expressiveness / coverage

Code generated performance

Feedback from Users


\subsection{Method}
\label{sec:method}

Adult Census Income Dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/adult}}

German Credit Dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Statlog+\%28German+Credit+Data\%29}}

Compas ProPublica Recidivism\footnote{\url{https://github.com/propublica/compas-analysis}}

\subsection{Results}
\label{sec:Results}

\section{Discussion}
\label{sec:discussion}

%\section{Related Work}
%\label{sec:related_work}

\section{Conclusions and Future Work}
\label{sec:conclusions_and_future_work}

Alfa \cite{cormen2009introduction}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% For citations use: 
%%       \citet{<label>} ==> Jones et al. [21]
%%       \citep{<label>} ==> [21]
%%

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
  \bibliographystyle{elsarticle-num-names} 
  \bibliography{references}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%\begin{thebibliography}{00}
%
%%% \bibitem[Author(year)]{label}
%%% Text of bibliographic item
%
%\bibitem[ ()]{}
%
%\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.
