@InProceedings{kamishima2012prejudice,
	author="Kamishima, Toshihiro
	and Akaho, Shotaro
	and Asoh, Hideki
	and Sakuma, Jun",
	editor="Flach, Peter A.
	and De Bie, Tijl
	and Cristianini, Nello",
	title="Fairness-Aware Classifier with Prejudice Remover Regularizer",
	booktitle="Machine Learning and Knowledge Discovery in Databases",
	year="2012",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="35--50",
	abstract="With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals' lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.",
	isbn="978-3-642-33486-3"
}


@inproceedings{pleiss2017equal,
	author = {Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {On Fairness and Calibration},
	url = {https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf},
	volume = {30},
	year = {2017}
}

@inproceedings{hardt2016equal,
	author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Equality of Opportunity in Supervised Learning},
	url = {https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
	volume = {29},
	year = {2016}
}

@INPROCEEDINGS{kamiran2012reject,  author={Kamiran, Faisal and Karim, Asim and Zhang, Xiangliang},  booktitle={2012 IEEE 12th International Conference on Data Mining},   title={Decision Theory for Discrimination-Aware Classification},   year={2012},  volume={},  number={},  pages={924-929},  doi={10.1109/ICDM.2012.45}}

@article{kamiran2011reweighing,
	author    = {Faisal Kamiran and
	Toon Calders},
	title     = {Data preprocessing techniques for classification without discrimination},
	journal   = {Knowl. Inf. Syst.},
	volume    = {33},
	number    = {1},
	pages     = {1--33},
	year      = {2011},
	url       = {https://doi.org/10.1007/s10115-011-0463-8},
	doi       = {10.1007/s10115-011-0463-8},
	timestamp = {Tue, 26 Jun 2018 14:10:08 +0200},
	biburl    = {https://dblp.org/rec/journals/kais/KamiranC11.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{feldman2015disparate,
	author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	title = {Certifying and Removing Disparate Impact},
	year = {2015},
	isbn = {9781450336642},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2783258.2783311},
	doi = {10.1145/2783258.2783311},
	abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses.We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
	booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {259â€“268},
	numpages = {10},
	keywords = {machine learning, fairness, disparate impact},
	location = {Sydney, NSW, Australia},
	series = {KDD '15}
}

@inproceedings{calmon2017optimized,
	author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Optimized Pre-Processing for Discrimination Prevention},
	url = {https://proceedings.neurips.cc/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf},
	volume = {30},
	year = {2017}
}

@InProceedings{zemel2013lfr,
	title = 	 {Learning Fair Representations},
	author = 	 {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
	booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
	pages = 	 {325--333},
	year = 	 {2013},
	editor = 	 {Dasgupta, Sanjoy and McAllester, David},
	volume = 	 {28},
	number =       {3},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Atlanta, Georgia, USA},
	month = 	 {17--19 Jun},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v28/zemel13.pdf},
	url = 	 {https://proceedings.mlr.press/v28/zemel13.html},
	abstract = 	 {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.}
}


@misc{zehlike2017fairness,
	title= {FAIRNESS MEASURES: A Platform for Data Collection and Benchmarking in discrimination-aware ML},
	author= {"Meike Zehlike and Carlos Castillo and Francesco Bonchi and Ricardo Baeza-Yates and Sara Hajian and Mohamed Megahed"},
	howpublished= {\url{https://fairnessmeasures.github.io}},
	month= "Jun",
	year= "2017",
	url="https://fairnessmeasures.github.io"
}

@inproceedings{speicher2018unified,
	author = {Speicher, Till and Heidari, Hoda and Grgic-Hlaca, Nina and Gummadi, Krishna P. and Singla, Adish and Weller, Adrian and Zafar, Muhammad Bilal},
	title = {A Unified Approach to Quantifying Algorithmic Unfairness: Measuring Individual and Group Unfairness via Inequality Indices},
	year = {2018},
	isbn = {9781450355520},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3219819.3220046},
	doi = {10.1145/3219819.3220046},
	abstract = {Discrimination via algorithmic decision making has received considerable attention. Prior work largely focuses on defining conditions for fairness, but does not define satisfactory measures of algorithmic unfairness. In this paper, we focus on the following question: Given two unfair algorithms, how should we determine which of the two is more unfair? Our core idea is to use existing inequality indices from economics to measure how unequally the outcomes of an algorithm benefit different individuals or groups in a population. Our work offers a justified and general framework to compare and contrast the (un)fairness of algorithmic predictors. This unifying approach enables us to quantify unfairness both at the individual and the group level. Further, our work reveals overlooked tradeoffs between different fairness notions: using our proposed measures, the overall individual-level unfairness of an algorithm can be decomposed into a between-group and a within-group component. Earlier methods are typically designed to tackle only between-group un- fairness, which may be justified for legal or other reasons. However, we demonstrate that minimizing exclusively the between-group component may, in fact, increase the within-group, and hence the overall unfairness. We characterize and illustrate the tradeoffs between our measures of (un)fairness and the prediction accuracy.},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {2239â€“2248},
	numpages = {10},
	keywords = {fairness in machine learning, inequality indices, fairness measures, generalized entropy, subgroup decomposability, algorithmic decision making, group fairness, individual fairness},
	location = {London, United Kingdom},
	series = {KDD '18}
}

@InProceedings{rogers2015using,
	author="Rogers, Benjamin
	and Qiao, Yechen
	and Gung, James
	and Mathur, Tanmay
	and Burge, Janet E.",
	editor="Gero, John S.
	and Hanna, Sean",
	title="Using Text Mining Techniques to Extract Rationale from Existing Documentation",
	booktitle="Design Computing and Cognition '14",
	year="2015",
	publisher="Springer International Publishing",
	address="Cham",
	pages="457--474",
	abstract="Software development and maintenance require making many decisions over the lifetime of the software. The decision problems, alternative solutions, and the arguments for and against these solutions comprise the system's rationale. This information is potentially valuable as a record of the developer and maintainers' intent. Unfortunately, this information is not explicitly captured in a structured form that can be easily analyzed. Still, while rationale is not explicitly captured, that does not mean that rationale is not captured at all---decisions are documented in many ways throughout the development process. This paper tackles the issue of extracting rationale from text by describing a mechanism for using two existing tools, GATE (General Architecture for Text Engineering) and WEKA (Waikato Environment for Knowledge Analysis) to build classification models for text mining of rationale. We used this mechanism to evaluate different combinations of text features and machine learning algorithms to extract rationale from Chrome bug reports. Our results are comparable in accuracy to those obtained by human annotators.",
	isbn="978-3-319-14956-1"
}

@article{bowen2009document,
	title={Document analysis as a qualitative research method},
	author={Bowen, Glenn A},
	journal={Qualitative research journal},
	year={2009},
	publisher={Emerald Group Publishing Limited},
	url={https://doi.org/10.3316/QRJ0902027}
}


@INPROCEEDINGS{poncin2011process,
	author={Poncin, Wouter and Serebrenik, Alexander and Brand, Mark van den},
	booktitle={2011 15th European Conference on Software Maintenance and Reengineering}, 
	title={Process Mining Software Repositories}, 
	year={2011},
	volume={},
	number={},
	pages={5-14},
	doi={10.1109/CSMR.2011.5}}

@MISC{conceicao2000theyoung,
	author = {Pedro ConceiÃ§Ã£o and Pedro Ferreira},
	title = {1The Young Personâ€™s Guide to the Theil Index: Suggesting Intuitive Interpretations and Exploring Analytical Applications},
	year = {2000}
}

@article{10.2307/2230396,
	author = {Johnston, J.},
	title = "{H. Theil. Economics and Information Theory}",
	journal = {The Economic Journal},
	volume = {79},
	number = {315},
	pages = {601-602},
	year = {1969},
	month = {09},
	issn = {0013-0133},
	doi = {10.2307/2230396},
	url = {https://doi.org/10.2307/2230396},
	eprint = {https://academic.oup.com/ej/article-pdf/79/315/601/27273392/ej0601.pdf},
}


@inproceedings{feldman2015disparate,
	author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	title = {Certifying and Removing Disparate Impact},
	year = {2015},
	isbn = {9781450336642},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2783258.2783311},
	doi = {10.1145/2783258.2783311},
	abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses.We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
	booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {259â€“268},
	numpages = {10},
	keywords = {fairness, machine learning, disparate impact},
	location = {Sydney, NSW, Australia},
	series = {KDD '15}
}

@inproceedings{dwork2012fairness,
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	title = {Fairness through Awareness},
	year = {2012},
	isbn = {9781450311151},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2090236.2090255},
	doi = {10.1145/2090236.2090255},
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
	pages = {214â€“226},
	numpages = {13},
	location = {Cambridge, Massachusetts},
	series = {ITCS '12}
}

@book{volter2013model,
	title={Model-Driven Software Development: Technology, Engineering, Management},
	author={V{\"o}lter, M. and Stahl, T. and Bettin, J. and Haase, A. and Helsen, S. and Czarnecki, K. and von Stockfleth, B.},
	isbn={9781118725764},
	lccn={2006007375},
	series={Wiley Software Patterns Series},
	url={https://books.google.co.uk/books?id=9ww\_D9fAKncC},
	year={2013},
	publisher={Wiley}
}

@book{brambilla2017model,
	title={Model-Driven Software Engineering in Practice},
	author={Brambilla, M. and Cabot, J. and Wimmer, M.},
	isbn={9781627057080},
	series={Synthesis Lectures on Software Engineering},
	url={https://books.google.co.uk/books?id=dHUuswEACAAJ},
	year={2017},
	publisher={Morgan \& Claypool Publishers}
}

@book{muller2016introduction,
	title={Introduction to Machine Learning with Python: A Guide for Data Scientists},
	author={M{\"u}ller, A.C. and Guido, S.},
	isbn={9781449369897},
	url={https://books.google.co.uk/books?id=vbQlDQAAQBAJ},
	year={2016},
	publisher={O'Reilly Media}
}

@book{byrne2017development,
	title={Development Workflows for Data Scientists},
	author={Byrne, C.},
	url={https://books.google.co.uk/books?id=84HgwQEACAAJ},
	year={2017},
	publisher={O'Reilly Media}
}

@inbook{lee2021landscape,
	author = {Lee, Michelle Seng Ah and Singh, Jat},
	title = {The Landscape and Gaps in Open Source Fairness Toolkits},
	year = {2021},
	isbn = {9781450380966},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3411764.3445261},
	abstract = { With the surge in literature focusing on the assessment and mitigation of unfair outcomes in algorithms, several open source â€˜fairness toolkitsâ€™ recently emerged to make such methods widely accessible. However, little studied are the differences in approach and capabilities of existing fairness toolkits, and their fit-for-purpose in commercial contexts. Towards this, this paper identifies the gaps between the existing open source fairness toolkit capabilities and the industry practitionersâ€™ needs. Specifically, we undertake a comparative assessment of the strengths and weaknesses of six prominent open source fairness toolkits, and investigate the current landscape and gaps in fairness toolkits through an exploratory focus group, a semi-structured interview, and an anonymous survey of data science/machine learning (ML) practitioners. We identify several gaps between the toolkitsâ€™ capabilities and practitioner needs, highlighting areas requiring attention and future directions towards tooling that better support â€˜fairness in practice.â€™},
	booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno = {699},
	numpages = {13}
}

@ARTICLE{googlewhatif2020,
	author={Wexler, James and Pushkarna, Mahima and Bolukbasi, Tolga and Wattenberg, Martin and ViÃ©gas, Fernanda and Wilson, Jimbo},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={The What-If Tool: Interactive Probing of Machine Learning Models}, 
	year={2020},
	volume={26},
	number={1},
	pages={56-65},
	doi={10.1109/TVCG.2019.2934619}}


@misc{saleiro2019aequitas,
	title={Aequitas: A Bias and Fairness Audit Toolkit}, 
	author={Pedro Saleiro and Benedict Kuester and Loren Hinkson and Jesse London and Abby Stevens and Ari Anisfeld and Kit T. Rodolfa and Rayid Ghani},
	year={2019},
	eprint={1811.05577},
	archivePrefix={arXiv},
	url={https://arxiv.org/abs/1811.05577},
	primaryClass={cs.LG}
}

@misc{aequitas2022,
	title={Aequitas},
	author={{Aequitas}},
	year={2022},
	url={http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/},
	note = {Accessed: 2022-01-30}
}


@misc{fairlearn2022,
	title={Improve fairness of AI systems},
	author={Fairlearn},
	year={2022},
	url={https://fairlearn.org/},
	note = {Accessed: 2022-01-30}
}

@misc{scikitlego2022,
	title={scikit-lego},
	author={{scikit-lego}},
	year={2022},
	url={https://scikit-lego.readthedocs.io/en/latest/index.html},
	note = {Accessed: 2022-01-30}
}

@misc{scikitfairness2022,
	title={scikit-fairness},
	author={{scikit-fairness}},
	year={2022},
	url={https://scikit-fairness.netlify.app/},
	note = {Accessed: 2022-01-30}
}

@techreport{bird2020fairlearn,
	author = {Bird, Sarah and Dud{\'i}k, Miro and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
	title = {Fairlearn: A toolkit for assessing and improving fairness in {AI}},
	institution = {Microsoft},
	year = {2020},
	month = {May},
	url = "https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/",
	number = {MSR-TR-2020-32},
}

@misc{ibmaifairness3602022,
	title={AI Fairness 360},
	author={{IBM Research Trusted AI}},
	year={2022},
	url={https://aif360.mybluemix.net/},
	note = {Accessed: 2022-01-30}
}

@misc{evans2017yaml,
	title={YAML Ainâ€™t Markup Language (YAMLâ„¢) Version 1.2.},
	author={Evans, Clark and Ben-Kiki, O and d{\"o}t Net, I},
	year={2017},
	url={https://yaml.org/spec/1.2.2},
	note = {Accessed: 2022-01-19}
}

@misc{angwin2016machine,
	title={Machine bias: Thereâ€™s software used across the country to predict future criminals. And itâ€™s biased against blacks. ProPublica (2016)},
	author={Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
	journal={Google Scholar},
	pages={23},
	year={2016},
	url={https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
	note = {Accessed: 2022-01-18}
}

@InProceedings{buolamwini2018gender,
	title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
	author = 	 {Buolamwini, Joy and Gebru, Timnit},
	booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
	pages = 	 {77--91},
	year = 	 {2018},
	editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
	volume = 	 {81},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {23--24 Feb},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
	url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
	abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}

@INPROCEEDINGS{lahoti2019ifair,
	author={Lahoti, Preethi and Gummadi, Krishna P. and Weikum, Gerhard},
	booktitle={2019 IEEE 35th International Conference on Data Engineering (ICDE)}, 
	title={iFair: Learning Individually Fair Data Representations for Algorithmic Decision Making}, 
	year={2019},
	volume={},
	number={},
	pages={1334-1345},
	doi={10.1109/ICDE.2019.00121}}

@inproceedings{chen2019fairness,
	author = {Chen, Jiahao and Kallus, Nathan and Mao, Xiaojie and Svacha, Geoffry and Udell, Madeleine},
	title = {Fairness Under Unawareness: Assessing Disparity When Protected Class Is Unobserved},
	year = {2019},
	isbn = {9781450361255},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3287560.3287594},
	doi = {10.1145/3287560.3287594},
	abstract = {Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.},
	booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	pages = {339â€“348},
	numpages = {10},
	keywords = {protected class, race imputation, racial discrimination, fair lending, Bayesian Improved Surname Geocoding, probablistic proxy model, disparate impact},
	location = {Atlanta, GA, USA},
	series = {FAT* '19}
}

 @misc{oxford2022bias,
 	author={{Oxford Reference}},
 	title = {Bias},
 	year = {2022},
 	url={https://www.oxfordreference.com/view/10.1093/oi/authority.20110803095504939},
 	note = {Accessed: 2022-01-16}
 }
 
@misc{bellamy2018ai,
	title={AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias}, 
	author={Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and Pranay Lohia and Jacquelyn Martino and Sameep Mehta and Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and John Richards and Diptikalyan Saha and Prasanna Sattigeri and Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
	year={2018},
	eprint={1810.01943},
	archivePrefix={arXiv},
	url={https://arxiv.org/abs/1810.01943},
	primaryClass={cs.AI}
}

@article{mehrabi2021survey,
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	title = {A Survey on Bias and Fairness in Machine Learning},
	year = {2021},
	issue_date = {July 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {54},
	number = {6},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3457607},
	doi = {10.1145/3457607},
	abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
	journal = {ACM Comput. Surv.},
	month = {jul},
	articleno = {115},
	numpages = {35},
	keywords = {machine learning, representation learning, deep learning, natural language processing, Fairness and bias in artificial intelligence}
}